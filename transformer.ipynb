{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b57d34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1125300d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81d9917",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5160341",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = sorted(set(''.join(text)))\n",
    "vocab_size = len(clean)\n",
    "stoi = { ch:i for i,ch in enumerate(clean) }\n",
    "itos = { i:ch for i,ch in enumerate(clean) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f5bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f4a227b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54977 parameters\n",
      "step 0: train loss 4.2215, val loss 4.2277\n",
      "step 100: train loss 3.1598, val loss 3.2041\n",
      "step 200: train loss 2.9248, val loss 2.9942\n",
      "step 300: train loss 2.7915, val loss 2.8178\n",
      "step 400: train loss 2.7588, val loss 2.7693\n",
      "step 500: train loss 2.6318, val loss 2.6854\n",
      "step 600: train loss 2.5813, val loss 2.5877\n",
      "step 700: train loss 2.5607, val loss 2.5921\n",
      "step 800: train loss 2.5678, val loss 2.5739\n",
      "step 900: train loss 2.5434, val loss 2.5625\n",
      "step 1000: train loss 2.5068, val loss 2.5376\n",
      "step 1100: train loss 2.4844, val loss 2.4988\n",
      "step 1200: train loss 2.4898, val loss 2.5026\n",
      "step 1300: train loss 2.5150, val loss 2.4933\n",
      "step 1400: train loss 2.4740, val loss 2.4913\n",
      "step 1500: train loss 2.4553, val loss 2.4668\n",
      "step 1600: train loss 2.5012, val loss 2.4856\n",
      "step 1700: train loss 2.4483, val loss 2.4475\n",
      "step 1800: train loss 2.4258, val loss 2.4428\n",
      "step 1900: train loss 2.3984, val loss 2.4348\n",
      "step 2000: train loss 2.4485, val loss 2.4016\n",
      "step 2100: train loss 2.4082, val loss 2.4492\n",
      "step 2200: train loss 2.4271, val loss 2.3769\n",
      "step 2300: train loss 2.3981, val loss 2.4283\n",
      "step 2400: train loss 2.3685, val loss 2.4241\n",
      "step 2500: train loss 2.3716, val loss 2.3812\n",
      "step 2600: train loss 2.3701, val loss 2.3681\n",
      "step 2700: train loss 2.3524, val loss 2.3956\n",
      "step 2800: train loss 2.3768, val loss 2.3997\n",
      "step 2900: train loss 2.3946, val loss 2.3920\n",
      "step 3000: train loss 2.3452, val loss 2.3556\n",
      "step 3100: train loss 2.3377, val loss 2.3558\n",
      "step 3200: train loss 2.3540, val loss 2.3751\n",
      "step 3300: train loss 2.3191, val loss 2.3457\n",
      "step 3400: train loss 2.3230, val loss 2.3345\n",
      "step 3500: train loss 2.3189, val loss 2.3486\n",
      "step 3600: train loss 2.3067, val loss 2.3197\n",
      "step 3700: train loss 2.3490, val loss 2.3282\n",
      "step 3800: train loss 2.3150, val loss 2.3453\n",
      "step 3900: train loss 2.3031, val loss 2.2736\n",
      "step 4000: train loss 2.3238, val loss 2.3274\n",
      "step 4100: train loss 2.2746, val loss 2.2969\n",
      "step 4200: train loss 2.2918, val loss 2.3183\n",
      "step 4300: train loss 2.2580, val loss 2.2722\n",
      "step 4400: train loss 2.3084, val loss 2.2934\n",
      "step 4500: train loss 2.2717, val loss 2.2759\n",
      "step 4600: train loss 2.2953, val loss 2.2774\n",
      "step 4700: train loss 2.2546, val loss 2.2680\n",
      "step 4800: train loss 2.2942, val loss 2.3147\n",
      "step 4900: train loss 2.2467, val loss 2.2774\n",
      "step 5000: train loss 2.2792, val loss 2.2614\n",
      "step 5100: train loss 2.2685, val loss 2.2718\n",
      "step 5200: train loss 2.2625, val loss 2.2487\n",
      "step 5300: train loss 2.2578, val loss 2.2254\n",
      "step 5400: train loss 2.2636, val loss 2.2815\n",
      "step 5500: train loss 2.2535, val loss 2.2828\n",
      "step 5600: train loss 2.2301, val loss 2.2508\n",
      "step 5700: train loss 2.2736, val loss 2.2705\n",
      "step 5800: train loss 2.2336, val loss 2.2694\n",
      "step 5900: train loss 2.2448, val loss 2.2616\n",
      "step 6000: train loss 2.2656, val loss 2.2308\n",
      "step 6100: train loss 2.2288, val loss 2.2642\n",
      "step 6200: train loss 2.2233, val loss 2.2216\n",
      "step 6300: train loss 2.2295, val loss 2.1993\n",
      "step 6400: train loss 2.2338, val loss 2.2691\n",
      "step 6500: train loss 2.2343, val loss 2.2344\n",
      "step 6600: train loss 2.2033, val loss 2.2576\n",
      "step 6700: train loss 2.2335, val loss 2.2616\n",
      "step 6800: train loss 2.2558, val loss 2.2245\n",
      "step 6900: train loss 2.2274, val loss 2.2768\n",
      "step 7000: train loss 2.2230, val loss 2.2209\n",
      "step 7100: train loss 2.2177, val loss 2.2180\n",
      "step 7200: train loss 2.1906, val loss 2.2521\n",
      "step 7300: train loss 2.2244, val loss 2.2366\n",
      "step 7400: train loss 2.1874, val loss 2.2313\n",
      "step 7500: train loss 2.1807, val loss 2.2283\n",
      "step 7600: train loss 2.1965, val loss 2.2114\n",
      "step 7700: train loss 2.1893, val loss 2.2196\n",
      "step 7800: train loss 2.1922, val loss 2.1869\n",
      "step 7900: train loss 2.1668, val loss 2.2041\n",
      "step 8000: train loss 2.1656, val loss 2.2100\n",
      "step 8100: train loss 2.1907, val loss 2.2261\n",
      "step 8200: train loss 2.1633, val loss 2.2139\n",
      "step 8300: train loss 2.1972, val loss 2.1913\n",
      "step 8400: train loss 2.1623, val loss 2.2524\n",
      "step 8500: train loss 2.1703, val loss 2.1883\n",
      "step 8600: train loss 2.1684, val loss 2.2004\n",
      "step 8700: train loss 2.1489, val loss 2.1955\n",
      "step 8800: train loss 2.1737, val loss 2.2238\n",
      "step 8900: train loss 2.1501, val loss 2.1820\n",
      "step 9000: train loss 2.1674, val loss 2.2145\n",
      "step 9100: train loss 2.1837, val loss 2.1716\n",
      "step 9200: train loss 2.1730, val loss 2.2341\n",
      "step 9300: train loss 2.1704, val loss 2.2149\n",
      "step 9400: train loss 2.1670, val loss 2.2072\n",
      "step 9500: train loss 2.1447, val loss 2.1918\n",
      "step 9600: train loss 2.1283, val loss 2.2111\n",
      "step 9700: train loss 2.1402, val loss 2.1758\n",
      "step 9800: train loss 2.1310, val loss 2.1904\n",
      "step 9900: train loss 2.1463, val loss 2.2006\n",
      "step 10000: train loss 2.1393, val loss 2.1876\n",
      "step 10100: train loss 2.1506, val loss 2.1977\n",
      "step 10200: train loss 2.1448, val loss 2.2035\n",
      "step 10300: train loss 2.1575, val loss 2.2089\n",
      "step 10400: train loss 2.1732, val loss 2.1975\n",
      "step 10500: train loss 2.1226, val loss 2.1701\n",
      "step 10600: train loss 2.1401, val loss 2.1562\n",
      "step 10700: train loss 2.1404, val loss 2.1523\n",
      "step 10800: train loss 2.1315, val loss 2.1925\n",
      "step 10900: train loss 2.1315, val loss 2.1690\n",
      "step 11000: train loss 2.1403, val loss 2.1585\n",
      "step 11100: train loss 2.1609, val loss 2.1413\n",
      "step 11200: train loss 2.1250, val loss 2.1514\n",
      "step 11300: train loss 2.1143, val loss 2.1674\n",
      "step 11400: train loss 2.1204, val loss 2.1745\n",
      "step 11500: train loss 2.1574, val loss 2.1715\n",
      "step 11600: train loss 2.1382, val loss 2.2151\n",
      "step 11700: train loss 2.1460, val loss 2.1657\n",
      "step 11800: train loss 2.0862, val loss 2.1373\n",
      "step 11900: train loss 2.1376, val loss 2.1692\n",
      "step 12000: train loss 2.1257, val loss 2.1688\n",
      "step 12100: train loss 2.1266, val loss 2.1464\n",
      "step 12200: train loss 2.1009, val loss 2.1781\n",
      "step 12300: train loss 2.1021, val loss 2.1328\n",
      "step 12400: train loss 2.1278, val loss 2.1867\n",
      "step 12500: train loss 2.1028, val loss 2.1403\n",
      "step 12600: train loss 2.0858, val loss 2.1856\n",
      "step 12700: train loss 2.1281, val loss 2.1637\n",
      "step 12800: train loss 2.1178, val loss 2.1627\n",
      "step 12900: train loss 2.1229, val loss 2.1854\n",
      "step 13000: train loss 2.1105, val loss 2.1683\n",
      "step 13100: train loss 2.1255, val loss 2.1533\n",
      "step 13200: train loss 2.0958, val loss 2.1300\n",
      "step 13300: train loss 2.0894, val loss 2.1860\n",
      "step 13400: train loss 2.0747, val loss 2.1515\n",
      "step 13500: train loss 2.0950, val loss 2.1755\n",
      "step 13600: train loss 2.1279, val loss 2.1382\n",
      "step 13700: train loss 2.1317, val loss 2.1592\n",
      "step 13800: train loss 2.1226, val loss 2.1584\n",
      "step 13900: train loss 2.1026, val loss 2.1952\n",
      "step 14000: train loss 2.1345, val loss 2.1560\n",
      "step 14100: train loss 2.1376, val loss 2.1584\n",
      "step 14200: train loss 2.1219, val loss 2.1895\n",
      "step 14300: train loss 2.1031, val loss 2.1906\n",
      "step 14400: train loss 2.0991, val loss 2.1102\n",
      "step 14500: train loss 2.1325, val loss 2.1585\n",
      "step 14600: train loss 2.1123, val loss 2.1267\n",
      "step 14700: train loss 2.0842, val loss 2.1507\n",
      "step 14800: train loss 2.0950, val loss 2.1156\n",
      "step 14900: train loss 2.0951, val loss 2.1654\n",
      "step 15000: train loss 2.1334, val loss 2.1682\n",
      "step 15100: train loss 2.1118, val loss 2.1335\n",
      "step 15200: train loss 2.0896, val loss 2.1373\n",
      "step 15300: train loss 2.0678, val loss 2.1390\n",
      "step 15400: train loss 2.1152, val loss 2.1793\n",
      "step 15500: train loss 2.1333, val loss 2.1768\n",
      "step 15600: train loss 2.1127, val loss 2.1643\n",
      "step 15700: train loss 2.1029, val loss 2.1552\n",
      "step 15800: train loss 2.0949, val loss 2.1865\n",
      "step 15900: train loss 2.0929, val loss 2.1591\n",
      "step 16000: train loss 2.0795, val loss 2.1839\n",
      "step 16100: train loss 2.0927, val loss 2.1302\n",
      "step 16200: train loss 2.0805, val loss 2.1436\n",
      "step 16300: train loss 2.1159, val loss 2.1782\n",
      "step 16400: train loss 2.0900, val loss 2.1369\n",
      "step 16500: train loss 2.0599, val loss 2.1560\n",
      "step 16600: train loss 2.0899, val loss 2.1633\n",
      "step 16700: train loss 2.0665, val loss 2.0976\n",
      "step 16800: train loss 2.0779, val loss 2.1001\n",
      "step 16900: train loss 2.1100, val loss 2.1016\n",
      "step 17000: train loss 2.0857, val loss 2.1126\n",
      "step 17100: train loss 2.1087, val loss 2.1362\n",
      "step 17200: train loss 2.0744, val loss 2.1361\n",
      "step 17300: train loss 2.0747, val loss 2.1459\n",
      "step 17400: train loss 2.0878, val loss 2.1402\n",
      "step 17500: train loss 2.0671, val loss 2.1455\n",
      "step 17600: train loss 2.0849, val loss 2.1653\n",
      "step 17700: train loss 2.0830, val loss 2.1073\n",
      "step 17800: train loss 2.0769, val loss 2.1565\n",
      "step 17900: train loss 2.1183, val loss 2.1343\n",
      "step 18000: train loss 2.0441, val loss 2.1208\n",
      "step 18100: train loss 2.0903, val loss 2.1252\n",
      "step 18200: train loss 2.0668, val loss 2.1116\n",
      "step 18300: train loss 2.0809, val loss 2.1401\n",
      "step 18400: train loss 2.0728, val loss 2.1277\n",
      "step 18500: train loss 2.0492, val loss 2.1201\n",
      "step 18600: train loss 2.0597, val loss 2.1432\n",
      "step 18700: train loss 2.0888, val loss 2.1508\n",
      "step 18800: train loss 2.0351, val loss 2.1300\n",
      "step 18900: train loss 2.0565, val loss 2.1109\n",
      "step 19000: train loss 2.0745, val loss 2.1158\n",
      "step 19100: train loss 2.0474, val loss 2.1190\n",
      "step 19200: train loss 2.0645, val loss 2.1336\n",
      "step 19300: train loss 2.0374, val loss 2.1711\n",
      "step 19400: train loss 2.0679, val loss 2.1390\n",
      "step 19500: train loss 2.0682, val loss 2.1222\n",
      "step 19600: train loss 2.0705, val loss 2.1263\n",
      "step 19700: train loss 2.0455, val loss 2.0835\n",
      "step 19800: train loss 2.0462, val loss 2.1203\n",
      "step 19900: train loss 2.0644, val loss 2.1245\n",
      "step 20000: train loss 2.0520, val loss 2.1219\n",
      "step 20100: train loss 2.0675, val loss 2.1503\n",
      "step 20200: train loss 2.0721, val loss 2.1221\n",
      "step 20300: train loss 2.0668, val loss 2.1134\n",
      "step 20400: train loss 2.0233, val loss 2.1571\n",
      "step 20500: train loss 2.0323, val loss 2.1156\n",
      "step 20600: train loss 2.0654, val loss 2.1438\n",
      "step 20700: train loss 2.0865, val loss 2.1770\n",
      "step 20800: train loss 2.0431, val loss 2.1215\n",
      "step 20900: train loss 2.0323, val loss 2.1346\n",
      "step 21000: train loss 2.0575, val loss 2.0981\n",
      "step 21100: train loss 2.0616, val loss 2.1209\n",
      "step 21200: train loss 2.0358, val loss 2.1178\n",
      "step 21300: train loss 2.0347, val loss 2.1045\n",
      "step 21400: train loss 2.0392, val loss 2.1115\n",
      "step 21500: train loss 2.0458, val loss 2.1421\n",
      "step 21600: train loss 2.0398, val loss 2.1395\n",
      "step 21700: train loss 2.0281, val loss 2.0692\n",
      "step 21800: train loss 2.0701, val loss 2.1458\n",
      "step 21900: train loss 2.0531, val loss 2.1124\n",
      "step 22000: train loss 2.0488, val loss 2.1181\n",
      "step 22100: train loss 2.0717, val loss 2.1243\n",
      "step 22200: train loss 2.0521, val loss 2.1037\n",
      "step 22300: train loss 2.0355, val loss 2.0943\n",
      "step 22400: train loss 2.0717, val loss 2.1111\n",
      "step 22500: train loss 2.0436, val loss 2.1027\n",
      "step 22600: train loss 2.0625, val loss 2.1165\n",
      "step 22700: train loss 2.0715, val loss 2.1057\n",
      "step 22800: train loss 2.0527, val loss 2.0884\n",
      "step 22900: train loss 2.0611, val loss 2.0788\n",
      "step 23000: train loss 2.0649, val loss 2.0747\n",
      "step 23100: train loss 2.0468, val loss 2.1291\n",
      "step 23200: train loss 2.0691, val loss 2.1040\n",
      "step 23300: train loss 2.0575, val loss 2.0790\n",
      "step 23400: train loss 2.0515, val loss 2.1074\n",
      "step 23500: train loss 2.0465, val loss 2.1211\n",
      "step 23600: train loss 2.0384, val loss 2.1008\n",
      "step 23700: train loss 2.0206, val loss 2.0997\n",
      "step 23800: train loss 2.0430, val loss 2.0725\n",
      "step 23900: train loss 2.0910, val loss 2.0956\n",
      "step 24000: train loss 2.0181, val loss 2.0698\n",
      "step 24100: train loss 2.0517, val loss 2.1185\n",
      "step 24200: train loss 2.0404, val loss 2.1158\n",
      "step 24300: train loss 2.0346, val loss 2.1187\n",
      "step 24400: train loss 2.0109, val loss 2.0878\n",
      "step 24500: train loss 2.0377, val loss 2.1214\n",
      "step 24600: train loss 2.0281, val loss 2.1071\n",
      "step 24700: train loss 2.0203, val loss 2.1013\n",
      "step 24800: train loss 2.0491, val loss 2.1234\n",
      "step 24900: train loss 2.0520, val loss 2.1239\n",
      "step 25000: train loss 2.0397, val loss 2.1285\n",
      "step 25100: train loss 2.0555, val loss 2.0938\n",
      "step 25200: train loss 2.0672, val loss 2.1211\n",
      "step 25300: train loss 2.0362, val loss 2.1173\n",
      "step 25400: train loss 2.0300, val loss 2.1029\n",
      "step 25500: train loss 2.0091, val loss 2.1084\n",
      "step 25600: train loss 2.0538, val loss 2.1259\n",
      "step 25700: train loss 2.0541, val loss 2.1026\n",
      "step 25800: train loss 2.0543, val loss 2.1138\n",
      "step 25900: train loss 2.0316, val loss 2.0890\n",
      "step 26000: train loss 2.0233, val loss 2.1323\n",
      "step 26100: train loss 2.0334, val loss 2.0812\n",
      "step 26200: train loss 2.0392, val loss 2.0991\n",
      "step 26300: train loss 2.0314, val loss 2.1106\n",
      "step 26400: train loss 2.0099, val loss 2.1267\n",
      "step 26500: train loss 2.0668, val loss 2.1143\n",
      "step 26600: train loss 1.9934, val loss 2.0981\n",
      "step 26700: train loss 2.0608, val loss 2.0951\n",
      "step 26800: train loss 2.0220, val loss 2.0684\n",
      "step 26900: train loss 2.0509, val loss 2.1028\n",
      "step 27000: train loss 2.0526, val loss 2.1258\n",
      "step 27100: train loss 2.0502, val loss 2.0969\n",
      "step 27200: train loss 2.0310, val loss 2.0993\n",
      "step 27300: train loss 2.0309, val loss 2.0949\n",
      "step 27400: train loss 2.0475, val loss 2.0780\n",
      "step 27500: train loss 2.0271, val loss 2.1069\n",
      "step 27600: train loss 2.0385, val loss 2.0833\n",
      "step 27700: train loss 2.0204, val loss 2.0871\n",
      "step 27800: train loss 2.0107, val loss 2.0968\n",
      "step 27900: train loss 2.0325, val loss 2.0994\n",
      "step 28000: train loss 2.0299, val loss 2.0795\n",
      "step 28100: train loss 2.0182, val loss 2.1268\n",
      "step 28200: train loss 2.0243, val loss 2.1128\n",
      "step 28300: train loss 2.0178, val loss 2.0775\n",
      "step 28400: train loss 1.9987, val loss 2.0722\n",
      "step 28500: train loss 2.0169, val loss 2.0727\n",
      "step 28600: train loss 2.0158, val loss 2.1037\n",
      "step 28700: train loss 2.0488, val loss 2.1085\n",
      "step 28800: train loss 1.9884, val loss 2.1045\n",
      "step 28900: train loss 1.9964, val loss 2.1124\n",
      "step 29000: train loss 2.0221, val loss 2.0992\n",
      "step 29100: train loss 1.9948, val loss 2.1111\n",
      "step 29200: train loss 2.0180, val loss 2.0638\n",
      "step 29300: train loss 2.0250, val loss 2.1251\n",
      "step 29400: train loss 1.9863, val loss 2.1185\n",
      "step 29500: train loss 2.0267, val loss 2.0693\n",
      "step 29600: train loss 2.0239, val loss 2.0914\n",
      "step 29700: train loss 2.0377, val loss 2.1052\n",
      "step 29800: train loss 2.0030, val loss 2.0868\n",
      "step 29900: train loss 2.0414, val loss 2.0920\n",
      "step 30000: train loss 2.0175, val loss 2.1007\n",
      "step 30100: train loss 2.0029, val loss 2.1008\n",
      "step 30200: train loss 2.0292, val loss 2.0991\n",
      "step 30300: train loss 2.0121, val loss 2.0714\n",
      "step 30400: train loss 2.0074, val loss 2.0498\n",
      "step 30500: train loss 2.0207, val loss 2.0914\n",
      "step 30600: train loss 1.9932, val loss 2.0874\n",
      "step 30700: train loss 2.0129, val loss 2.0737\n",
      "step 30800: train loss 2.0371, val loss 2.1209\n",
      "step 30900: train loss 1.9415, val loss 2.1211\n",
      "step 31000: train loss 2.0344, val loss 2.1065\n",
      "step 31100: train loss 2.0124, val loss 2.0808\n",
      "step 31200: train loss 2.0272, val loss 2.0738\n",
      "step 31300: train loss 2.0002, val loss 2.0977\n",
      "step 31400: train loss 2.0535, val loss 2.0935\n",
      "step 31500: train loss 2.0202, val loss 2.0886\n",
      "step 31600: train loss 1.9861, val loss 2.0385\n",
      "step 31700: train loss 2.0157, val loss 2.0921\n",
      "step 31800: train loss 2.0340, val loss 2.0742\n",
      "step 31900: train loss 1.9791, val loss 2.1064\n",
      "step 32000: train loss 1.9780, val loss 2.0976\n",
      "step 32100: train loss 2.0068, val loss 2.0830\n",
      "step 32200: train loss 1.9943, val loss 2.0707\n",
      "step 32300: train loss 2.0356, val loss 2.0912\n",
      "step 32400: train loss 1.9975, val loss 2.0758\n",
      "step 32500: train loss 1.9706, val loss 2.0935\n",
      "step 32600: train loss 2.0238, val loss 2.1183\n",
      "step 32700: train loss 1.9992, val loss 2.0760\n",
      "step 32800: train loss 1.9640, val loss 2.1198\n",
      "step 32900: train loss 2.0372, val loss 2.0652\n",
      "step 33000: train loss 1.9886, val loss 2.0708\n",
      "step 33100: train loss 2.0278, val loss 2.1120\n",
      "step 33200: train loss 2.0111, val loss 2.1039\n",
      "step 33300: train loss 1.9967, val loss 2.0790\n",
      "step 33400: train loss 1.9709, val loss 2.0511\n",
      "step 33500: train loss 1.9867, val loss 2.0932\n",
      "step 33600: train loss 2.0025, val loss 2.0945\n",
      "step 33700: train loss 1.9973, val loss 2.0912\n",
      "step 33800: train loss 1.9966, val loss 2.0922\n",
      "step 33900: train loss 1.9842, val loss 2.0462\n",
      "step 34000: train loss 1.9954, val loss 2.1022\n",
      "step 34100: train loss 2.0006, val loss 2.0775\n",
      "step 34200: train loss 2.0064, val loss 2.0711\n",
      "step 34300: train loss 2.0314, val loss 2.0897\n",
      "step 34400: train loss 2.0040, val loss 2.0752\n",
      "step 34500: train loss 1.9989, val loss 2.0577\n",
      "step 34600: train loss 1.9970, val loss 2.0898\n",
      "step 34700: train loss 2.0346, val loss 2.0730\n",
      "step 34800: train loss 1.9786, val loss 2.1285\n",
      "step 34900: train loss 2.0053, val loss 2.0743\n",
      "step 35000: train loss 1.9943, val loss 2.0471\n",
      "step 35100: train loss 1.9837, val loss 2.0606\n",
      "step 35200: train loss 2.0009, val loss 2.0494\n",
      "step 35300: train loss 2.0141, val loss 2.0875\n",
      "step 35400: train loss 1.9898, val loss 2.0585\n",
      "step 35500: train loss 1.9967, val loss 2.0751\n",
      "step 35600: train loss 1.9793, val loss 2.0861\n",
      "step 35700: train loss 1.9849, val loss 2.0875\n",
      "step 35800: train loss 1.9958, val loss 2.1048\n",
      "step 35900: train loss 1.9860, val loss 2.0739\n",
      "step 36000: train loss 2.0123, val loss 2.0607\n",
      "step 36100: train loss 1.9804, val loss 2.0730\n",
      "step 36200: train loss 1.9907, val loss 2.1100\n",
      "step 36300: train loss 1.9864, val loss 2.0909\n",
      "step 36400: train loss 2.0496, val loss 2.0929\n",
      "step 36500: train loss 2.0340, val loss 2.1026\n",
      "step 36600: train loss 2.0205, val loss 2.0360\n",
      "step 36700: train loss 1.9685, val loss 2.0691\n",
      "step 36800: train loss 1.9782, val loss 2.0426\n",
      "step 36900: train loss 1.9916, val loss 2.0971\n",
      "step 37000: train loss 1.9958, val loss 2.0978\n",
      "step 37100: train loss 1.9748, val loss 2.0859\n",
      "step 37200: train loss 1.9827, val loss 2.0856\n",
      "step 37300: train loss 1.9927, val loss 2.0959\n",
      "step 37400: train loss 1.9760, val loss 2.1070\n",
      "step 37500: train loss 1.9912, val loss 2.0652\n",
      "step 37600: train loss 2.0121, val loss 2.0676\n",
      "step 37700: train loss 1.9840, val loss 2.0728\n",
      "step 37800: train loss 1.9923, val loss 2.0935\n",
      "step 37900: train loss 2.0214, val loss 2.0857\n",
      "step 38000: train loss 1.9926, val loss 2.0606\n",
      "step 38100: train loss 1.9922, val loss 2.0821\n",
      "step 38200: train loss 2.0106, val loss 2.0377\n",
      "step 38300: train loss 1.9921, val loss 2.0340\n",
      "step 38400: train loss 1.9959, val loss 2.0447\n",
      "step 38500: train loss 1.9616, val loss 2.0826\n",
      "step 38600: train loss 1.9830, val loss 2.0932\n",
      "step 38700: train loss 2.0124, val loss 2.0653\n",
      "step 38800: train loss 1.9947, val loss 2.0666\n",
      "step 38900: train loss 1.9850, val loss 2.0720\n",
      "step 39000: train loss 1.9627, val loss 2.0644\n",
      "step 39100: train loss 1.9883, val loss 2.0406\n",
      "step 39200: train loss 1.9914, val loss 2.1206\n",
      "step 39300: train loss 2.0110, val loss 2.0719\n",
      "step 39400: train loss 2.0164, val loss 2.0948\n",
      "step 39500: train loss 1.9755, val loss 2.0726\n",
      "step 39600: train loss 1.9704, val loss 2.1047\n",
      "step 39700: train loss 1.9567, val loss 2.0669\n",
      "step 39800: train loss 1.9994, val loss 2.0504\n",
      "step 39900: train loss 1.9568, val loss 2.0659\n",
      "step 40000: train loss 1.9874, val loss 2.0604\n",
      "step 40100: train loss 1.9843, val loss 2.0827\n",
      "step 40200: train loss 1.9857, val loss 2.0585\n",
      "step 40300: train loss 2.0103, val loss 2.0446\n",
      "step 40400: train loss 2.0112, val loss 2.0701\n",
      "step 40500: train loss 1.9971, val loss 2.0636\n",
      "step 40600: train loss 1.9837, val loss 2.0927\n",
      "step 40700: train loss 1.9954, val loss 2.0870\n",
      "step 40800: train loss 1.9952, val loss 2.0583\n",
      "step 40900: train loss 1.9858, val loss 2.0780\n",
      "step 41000: train loss 1.9462, val loss 2.0872\n",
      "step 41100: train loss 1.9766, val loss 2.0759\n",
      "step 41200: train loss 1.9891, val loss 2.0518\n",
      "step 41300: train loss 2.0238, val loss 2.0772\n",
      "step 41400: train loss 1.9590, val loss 2.0525\n",
      "step 41500: train loss 1.9740, val loss 2.0531\n",
      "step 41600: train loss 1.9993, val loss 2.0801\n",
      "step 41700: train loss 1.9916, val loss 2.0346\n",
      "step 41800: train loss 2.0005, val loss 2.0639\n",
      "step 41900: train loss 2.0129, val loss 2.1056\n",
      "step 42000: train loss 1.9978, val loss 2.0552\n",
      "step 42100: train loss 2.0044, val loss 2.0708\n",
      "step 42200: train loss 1.9700, val loss 2.0909\n",
      "step 42300: train loss 2.0029, val loss 2.0562\n",
      "step 42400: train loss 1.9902, val loss 2.0920\n",
      "step 42500: train loss 1.9806, val loss 2.0650\n",
      "step 42600: train loss 1.9720, val loss 2.0397\n",
      "step 42700: train loss 1.9580, val loss 2.0706\n",
      "step 42800: train loss 1.9928, val loss 2.0824\n",
      "step 42900: train loss 2.0108, val loss 2.0195\n",
      "step 43000: train loss 1.9926, val loss 2.0581\n",
      "step 43100: train loss 1.9915, val loss 2.0811\n",
      "step 43200: train loss 1.9425, val loss 2.0629\n",
      "step 43300: train loss 1.9418, val loss 2.0289\n",
      "step 43400: train loss 1.9641, val loss 2.0831\n",
      "step 43500: train loss 2.0000, val loss 2.0703\n",
      "step 43600: train loss 1.9709, val loss 2.1105\n",
      "step 43700: train loss 1.9935, val loss 2.0589\n",
      "step 43800: train loss 1.9850, val loss 2.0774\n",
      "step 43900: train loss 2.0065, val loss 2.0706\n",
      "step 44000: train loss 1.9824, val loss 2.1088\n",
      "step 44100: train loss 1.9574, val loss 2.0741\n",
      "step 44200: train loss 1.9806, val loss 2.0586\n",
      "step 44300: train loss 2.0134, val loss 2.0501\n",
      "step 44400: train loss 1.9932, val loss 2.0561\n",
      "step 44500: train loss 1.9616, val loss 2.0531\n",
      "step 44600: train loss 1.9987, val loss 2.0312\n",
      "step 44700: train loss 1.9692, val loss 2.0815\n",
      "step 44800: train loss 1.9727, val loss 2.0729\n",
      "step 44900: train loss 1.9793, val loss 2.0701\n",
      "step 45000: train loss 1.9759, val loss 2.0541\n",
      "step 45100: train loss 1.9423, val loss 2.0677\n",
      "step 45200: train loss 1.9689, val loss 2.0449\n",
      "step 45300: train loss 1.9914, val loss 2.0430\n",
      "step 45400: train loss 1.9939, val loss 2.0710\n",
      "step 45500: train loss 1.9635, val loss 2.0591\n",
      "step 45600: train loss 1.9733, val loss 2.0829\n",
      "step 45700: train loss 2.0048, val loss 2.0668\n",
      "step 45800: train loss 1.9965, val loss 2.0672\n",
      "step 45900: train loss 1.9456, val loss 2.0965\n",
      "step 46000: train loss 1.9521, val loss 2.0468\n",
      "step 46100: train loss 1.9761, val loss 2.0752\n",
      "step 46200: train loss 1.9881, val loss 2.0538\n",
      "step 46300: train loss 1.9830, val loss 2.0657\n",
      "step 46400: train loss 1.9759, val loss 2.0802\n",
      "step 46500: train loss 1.9479, val loss 2.0566\n",
      "step 46600: train loss 1.9981, val loss 2.0317\n",
      "step 46700: train loss 2.0249, val loss 2.0529\n",
      "step 46800: train loss 2.0018, val loss 2.0397\n",
      "step 46900: train loss 1.9475, val loss 2.0550\n",
      "step 47000: train loss 1.9756, val loss 2.0472\n",
      "step 47100: train loss 1.9598, val loss 2.0528\n",
      "step 47200: train loss 1.9582, val loss 2.0696\n",
      "step 47300: train loss 2.0041, val loss 2.0596\n",
      "step 47400: train loss 1.9562, val loss 2.0867\n",
      "step 47500: train loss 1.9665, val loss 2.0124\n",
      "step 47600: train loss 1.9823, val loss 2.0517\n",
      "step 47700: train loss 1.9795, val loss 2.0744\n",
      "step 47800: train loss 1.9634, val loss 2.0756\n",
      "step 47900: train loss 1.9705, val loss 2.0363\n",
      "step 48000: train loss 1.9674, val loss 2.0317\n",
      "step 48100: train loss 1.9720, val loss 2.0855\n",
      "step 48200: train loss 1.9833, val loss 2.0784\n",
      "step 48300: train loss 1.9975, val loss 2.0516\n",
      "step 48400: train loss 1.9856, val loss 2.0150\n",
      "step 48500: train loss 1.9646, val loss 2.0501\n",
      "step 48600: train loss 1.9521, val loss 2.0574\n",
      "step 48700: train loss 1.9545, val loss 2.0289\n",
      "step 48800: train loss 1.9487, val loss 2.0456\n",
      "step 48900: train loss 1.9426, val loss 2.0769\n",
      "step 49000: train loss 1.9674, val loss 2.0769\n",
      "step 49100: train loss 1.9596, val loss 2.0552\n",
      "step 49200: train loss 1.9644, val loss 2.0517\n",
      "step 49300: train loss 1.9326, val loss 2.0229\n",
      "step 49400: train loss 1.9772, val loss 2.0828\n",
      "step 49500: train loss 1.9442, val loss 2.0938\n",
      "step 49600: train loss 1.9443, val loss 2.0411\n",
      "step 49700: train loss 1.9536, val loss 2.0683\n",
      "step 49800: train loss 1.9644, val loss 2.0008\n",
      "step 49900: train loss 1.9328, val loss 2.0564\n",
      "step 49999: train loss 1.9611, val loss 2.0613\n",
      "\n",
      "no: in thin:\n",
      "Than mightousor, hat driflenely.\n",
      "Suitt:\n",
      "Mownctjussichy, I thee has; in that tightill kinger whas I roph Capon at:\n",
      "Coudier\n",
      "Weent thare farth por thrence,\n",
      "Thee a man,\n",
      "Now mut live eart a neseln:\n",
      "To souse note his binge \n",
      "For, madUth but our for's mear,\n",
      "Which jaus, fear.\n",
      "\n",
      "\n",
      "DUCHES:\n",
      "Whal; rethither,\n",
      "My thu maish of an;\n",
      "But Recher, that our luch serupaun pleor? Friuse have in peates stahe shee docn\n",
      "Lord.\n",
      "Goou  will you?\n",
      "He thing, ay, thoughter that drioiniilg thim, smalteNT I\n",
      "Dhall prester\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "#hyper parameter\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "n_embd = 32\n",
    "dropout = 0.2\n",
    "n_layers = 4\n",
    "num_heads = 4\n",
    "learning_rate = 1e-3\n",
    "max_iters = 50000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "\n",
    "with open('datasets/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "clean = sorted(set(''.join(text)))\n",
    "vocab_size = len(clean)\n",
    "stoi = { ch:i for i,ch in enumerate(clean) }\n",
    "itos = { i:ch for i,ch in enumerate(clean) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    idx = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "    return x,y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters, device=device)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        wei = q @ k.transpose(-2,-1) * self.head_size **-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        return wei @ v\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // num_heads\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out)) \n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ffd = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "           nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.ffd(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.sa = MultiHeadAttention(num_heads)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.FeedForward = FeedForward()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.FeedForward(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(num_heads) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.final_ln = nn.LayerNorm(n_embd)\n",
    "        self.final_linear = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        # x is a tensor of shape (B, T), where B is the batch size and T is the sequence length\n",
    "        B, T = x.shape\n",
    "        \n",
    "        # tok_embd should have shape (B, T, n_embd)\n",
    "        tok_embd = self.token_embedding_table(x)  # (B, T, n_embd)\n",
    "        pos_embd = self.position_embedding_table(torch.arange(T, device=x.device))  # (T, n_embd)\n",
    "        x = tok_embd + pos_embd #B, T, n_embd\n",
    "        x = self.blocks(x)\n",
    "        logits = self.final_linear(self.final_ln(x)) #(B, T, vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = x[:, -block_size:] #last 8 tokens only if more that 8..\n",
    "            logits, loss = self(idx_cond) # logits: (B, T, vocab_size) → (1, 1, 65) at first\n",
    "            logits = logits[:, -1, :] # focus only on the last time step # becomes (1,65)\n",
    "            probs = F.softmax(logits, dim=-1) # shape: (1, 65)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (1, 1)\n",
    "            x = torch.cat((x, idx_next), dim=1) # shape grows → (1, 2), (1, 3), ..., (1, T+max_new_tokens)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "model = GPT().to(device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters()), 'parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "start = torch.zeros((1,1), dtype=torch.long, device=device)  # BOS = index 0 char if that's ' '\n",
    "out_idx = model.generate(start, max_new_tokens=500)\n",
    "print(decode(out_idx[0].tolist()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef478a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the ker storry o' 'as whila, loed holoy of my seath whantes rent.\n",
      "\n",
      "KIARD mesten?\n",
      "\n",
      "KING HENREN:\n",
      "Are a se'cher, of will.\n",
      "Yoorh,\n",
      "Eneshard. Or kaws, swaved warws gendivers,\n",
      "For Oor by though me; fore it\n",
      "s,\n",
      "Dhes necronn at,hich\n",
      "Thint; of you of are is time hill of?\n",
      "As this the maursife my soo face of blige Soweins Lord,\n",
      "stis; dow I wall crieas thee so' chan Bleepber hand, then,\n",
      "Faren,\n",
      "The pophale.-\n",
      "Hasnilse.\n",
      "And the thatt our will his this scone as king than necher angend shalk a ble yous do clans on crace 'tend.\n",
      "His Mordilst! hith, emwan rom of my hen.\n",
      "\n",
      "Cawentwe the yeec Earth retonon;\n",
      "Thinqely here say is.\n",
      "\n",
      "DADIAULINLAUS:\n",
      "And and illantle, Kose,\n",
      "Wheat nispranubothin in resecough, not tentlusuce.\n",
      "And;\n",
      "Fear my kill of wicke,\n",
      "Show.\n",
      "Dhe.\n",
      "For nowbllIqUek whup\n",
      "As seeep-tent; o nocesom hather naite of\n",
      "Hey if make the mad a causee of must and. You him, and, dyay, his smea see's I doms, in prinomble than oum not of Cave in man Is forn'd pation our, andled you spear my Curlseat thife the letcherm d\n"
     ]
    }
   ],
   "source": [
    "start = torch.zeros((1,1), dtype=torch.long, device=device)  # BOS = index 0 char if that's ' '\n",
    "out_idx = model.generate(start, max_new_tokens=1000)\n",
    "print(decode(out_idx[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62189281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
